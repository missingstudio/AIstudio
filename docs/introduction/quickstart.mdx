---
title: "Quickstart"
description: "Quickly start deploying **a production-ready core infrastructure stack** for your LLM applications"
icon: "bolt"
---

This guide will walk you through setting up a production-ready core infrastructure stack for your LLM Application with minimal effort. 
In just a few steps, you'll be able to setup Universal API, AI routing, AI Gateway and Observablity to track and analyze the performance 
and usage of your Large Language Model (LLM) applications.

<Steps>
  <Step title="Deploy AI studio">
    #### Preparing docker environment
    ```yaml docker-compose.yml
      version: '3.8'

      services:
        postgres:
          image: postgres:13
          environment:
            - POSTGRES_DB=aistudio
            - POSTGRES_USER=aistudio
            - POSTGRES_HOST_AUTH_METHOD=trust
          volumes:
            - postgres:/var/lib/postgresql/data
          ports:
            - 5432:5432
          healthcheck:
            test: [ "CMD-SHELL", "pg_isready -U aistudio" ]
            interval: 30s
            timeout: 30s
            retries: 3
          restart: always

        redis:
          image: redis:7.2-alpine
          ports:
            - 6379:6379
          command: redis-server --save 20 1 --loglevel warning
          volumes:
            - redis:/data
          healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 1s
            timeout: 3s
            retries: 30
          restart: always

        clickhouse:
          image: clickhouse/clickhouse-server:24.1.5
          volumes:
            - clickhouse:/var/lib/clickhouse
          ports:
            - 9000:9000
            - 8123:8123
          restart: always

        migrate:
          image: missingstudio/ai:dev
          command: ["migrate"]
          environment:
            - GATEWAY_POSTGRES_URL=postgres://aistudio@postgres:5432/aistudio?sslmode=disable
          depends_on:
            - postgres

        aistudio:
          image: missingstudio/ai:dev
          environment:
            - GATEWAY_APP_HOST=0.0.0.0
            - GATEWAY_APP_PORT=8080
            - GATEWAY_REDIS_HOST=redis
            - GATEWAY_REDIS_PORT=6379
            - GATEWAY_POSTGRES_URL=postgres://aistudio@postgres:5432/aistudio?sslmode=disable
          command: ["start"]
          ports:
            - 8080:8080
          depends_on:
            - redis
            - postgres
            - clickhouse
            - migrate

      volumes:
        redis: {}
        postgres: {}
        clickhouse: {}
    ```

    #### Launch AI studio gateway server
    ```shell
      docker-compose up -d
    ```

  </Step>
  <Step title="Generate an API Key">
    With AI studio running, the next step is to [generate an API key](/api-reference/endpoint/api-keys/create) for resource access from AI studio

    To generate your first API key, you can use the following command:

    ```shell shell
    curl --request POST \
      --url http://127.0.0.1:8080/api/v1/keys \
      --header 'Content-Type: application/json' \
      --data '{
        "name": "Base"
      }'
    ```

    <Note>
    **Save the API Key**
    Remember to include this API key in the `X-Ms-Api-Key` header for all future API interactions
    </Note>

  </Step>
</Steps>

**You're geared up and ready to go! ðŸš€**

Following these steps should have you AI studio up and running to power up LLMOps for your LLM appplications.
If you have any questions or need support, reach out to our [Discord Community](https://discord.gg/yxV58ydzV7).

---

<CardGroup cols={2}>
  <Card title="Providers" href="/providers/introduction" icon="circle-nodes">
    Integrate your LLM Provider with AI studio
  </Card>
  <Card title="Connections" href="/connections/introduction" icon="link">
    Connect to your existing Observablity Platform
  </Card>
</CardGroup>
