// Code generated by protoc-gen-connect-go. DO NOT EDIT.
//
// Source: llm/service.proto

package llmv1beta1connect

import (
	connect "connectrpc.com/connect"
	context "context"
	errors "errors"
	llm "github.com/missingstudio/studio/protos/pkg/llm"
	http "net/http"
	strings "strings"
)

// This is a compile-time assertion to ensure that this generated file and the connect package are
// compatible. If you get a compiler error that this constant is not defined, this code was
// generated with a version of connect newer than the one compiled into your binary. You can fix the
// problem by either regenerating this code with an older version of connect or updating the connect
// version compiled into your binary.
const _ = connect.IsAtLeastVersion1_13_0

const (
	// LLMServiceName is the fully-qualified name of the LLMService service.
	LLMServiceName = "llm.v1beta1.LLMService"
)

// These constants are the fully-qualified names of the RPCs defined in this package. They're
// exposed at runtime as Spec.Procedure and as the final two segments of the HTTP route.
//
// Note that these are different from the fully-qualified method names used by
// google.golang.org/protobuf/reflect/protoreflect. To convert from these constants to
// reflection-formatted method names, remove the leading slash and convert the remaining slash to a
// period.
const (
	// LLMServiceChatProcedure is the fully-qualified name of the LLMService's Chat RPC.
	LLMServiceChatProcedure = "/llm.v1beta1.LLMService/Chat"
	// LLMServiceCompletionProcedure is the fully-qualified name of the LLMService's Completion RPC.
	LLMServiceCompletionProcedure = "/llm.v1beta1.LLMService/Completion"
	// LLMServiceLoadModelProcedure is the fully-qualified name of the LLMService's LoadModel RPC.
	LLMServiceLoadModelProcedure = "/llm.v1beta1.LLMService/LoadModel"
)

// These variables are the protoreflect.Descriptor objects for the RPCs defined in this package.
var (
	lLMServiceServiceDescriptor          = llm.File_llm_service_proto.Services().ByName("LLMService")
	lLMServiceChatMethodDescriptor       = lLMServiceServiceDescriptor.Methods().ByName("Chat")
	lLMServiceCompletionMethodDescriptor = lLMServiceServiceDescriptor.Methods().ByName("Completion")
	lLMServiceLoadModelMethodDescriptor  = lLMServiceServiceDescriptor.Methods().ByName("LoadModel")
)

// LLMServiceClient is a client for the llm.v1beta1.LLMService service.
type LLMServiceClient interface {
	Chat(context.Context, *connect.Request[llm.ChatRequest]) (*connect.Response[llm.ChatPrediction], error)
	Completion(context.Context, *connect.Request[llm.CompletionRequest]) (*connect.Response[llm.CompletionPrediction], error)
	LoadModel(context.Context, *connect.Request[llm.LoadModelRequest]) (*connect.Response[llm.LoadModelResponse], error)
}

// NewLLMServiceClient constructs a client for the llm.v1beta1.LLMService service. By default, it
// uses the Connect protocol with the binary Protobuf Codec, asks for gzipped responses, and sends
// uncompressed requests. To use the gRPC or gRPC-Web protocols, supply the connect.WithGRPC() or
// connect.WithGRPCWeb() options.
//
// The URL supplied here should be the base URL for the Connect or gRPC server (for example,
// http://api.acme.com or https://acme.com/grpc).
func NewLLMServiceClient(httpClient connect.HTTPClient, baseURL string, opts ...connect.ClientOption) LLMServiceClient {
	baseURL = strings.TrimRight(baseURL, "/")
	return &lLMServiceClient{
		chat: connect.NewClient[llm.ChatRequest, llm.ChatPrediction](
			httpClient,
			baseURL+LLMServiceChatProcedure,
			connect.WithSchema(lLMServiceChatMethodDescriptor),
			connect.WithClientOptions(opts...),
		),
		completion: connect.NewClient[llm.CompletionRequest, llm.CompletionPrediction](
			httpClient,
			baseURL+LLMServiceCompletionProcedure,
			connect.WithSchema(lLMServiceCompletionMethodDescriptor),
			connect.WithClientOptions(opts...),
		),
		loadModel: connect.NewClient[llm.LoadModelRequest, llm.LoadModelResponse](
			httpClient,
			baseURL+LLMServiceLoadModelProcedure,
			connect.WithSchema(lLMServiceLoadModelMethodDescriptor),
			connect.WithClientOptions(opts...),
		),
	}
}

// lLMServiceClient implements LLMServiceClient.
type lLMServiceClient struct {
	chat       *connect.Client[llm.ChatRequest, llm.ChatPrediction]
	completion *connect.Client[llm.CompletionRequest, llm.CompletionPrediction]
	loadModel  *connect.Client[llm.LoadModelRequest, llm.LoadModelResponse]
}

// Chat calls llm.v1beta1.LLMService.Chat.
func (c *lLMServiceClient) Chat(ctx context.Context, req *connect.Request[llm.ChatRequest]) (*connect.Response[llm.ChatPrediction], error) {
	return c.chat.CallUnary(ctx, req)
}

// Completion calls llm.v1beta1.LLMService.Completion.
func (c *lLMServiceClient) Completion(ctx context.Context, req *connect.Request[llm.CompletionRequest]) (*connect.Response[llm.CompletionPrediction], error) {
	return c.completion.CallUnary(ctx, req)
}

// LoadModel calls llm.v1beta1.LLMService.LoadModel.
func (c *lLMServiceClient) LoadModel(ctx context.Context, req *connect.Request[llm.LoadModelRequest]) (*connect.Response[llm.LoadModelResponse], error) {
	return c.loadModel.CallUnary(ctx, req)
}

// LLMServiceHandler is an implementation of the llm.v1beta1.LLMService service.
type LLMServiceHandler interface {
	Chat(context.Context, *connect.Request[llm.ChatRequest]) (*connect.Response[llm.ChatPrediction], error)
	Completion(context.Context, *connect.Request[llm.CompletionRequest]) (*connect.Response[llm.CompletionPrediction], error)
	LoadModel(context.Context, *connect.Request[llm.LoadModelRequest]) (*connect.Response[llm.LoadModelResponse], error)
}

// NewLLMServiceHandler builds an HTTP handler from the service implementation. It returns the path
// on which to mount the handler and the handler itself.
//
// By default, handlers support the Connect, gRPC, and gRPC-Web protocols with the binary Protobuf
// and JSON codecs. They also support gzip compression.
func NewLLMServiceHandler(svc LLMServiceHandler, opts ...connect.HandlerOption) (string, http.Handler) {
	lLMServiceChatHandler := connect.NewUnaryHandler(
		LLMServiceChatProcedure,
		svc.Chat,
		connect.WithSchema(lLMServiceChatMethodDescriptor),
		connect.WithHandlerOptions(opts...),
	)
	lLMServiceCompletionHandler := connect.NewUnaryHandler(
		LLMServiceCompletionProcedure,
		svc.Completion,
		connect.WithSchema(lLMServiceCompletionMethodDescriptor),
		connect.WithHandlerOptions(opts...),
	)
	lLMServiceLoadModelHandler := connect.NewUnaryHandler(
		LLMServiceLoadModelProcedure,
		svc.LoadModel,
		connect.WithSchema(lLMServiceLoadModelMethodDescriptor),
		connect.WithHandlerOptions(opts...),
	)
	return "/llm.v1beta1.LLMService/", http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		switch r.URL.Path {
		case LLMServiceChatProcedure:
			lLMServiceChatHandler.ServeHTTP(w, r)
		case LLMServiceCompletionProcedure:
			lLMServiceCompletionHandler.ServeHTTP(w, r)
		case LLMServiceLoadModelProcedure:
			lLMServiceLoadModelHandler.ServeHTTP(w, r)
		default:
			http.NotFound(w, r)
		}
	})
}

// UnimplementedLLMServiceHandler returns CodeUnimplemented from all methods.
type UnimplementedLLMServiceHandler struct{}

func (UnimplementedLLMServiceHandler) Chat(context.Context, *connect.Request[llm.ChatRequest]) (*connect.Response[llm.ChatPrediction], error) {
	return nil, connect.NewError(connect.CodeUnimplemented, errors.New("llm.v1beta1.LLMService.Chat is not implemented"))
}

func (UnimplementedLLMServiceHandler) Completion(context.Context, *connect.Request[llm.CompletionRequest]) (*connect.Response[llm.CompletionPrediction], error) {
	return nil, connect.NewError(connect.CodeUnimplemented, errors.New("llm.v1beta1.LLMService.Completion is not implemented"))
}

func (UnimplementedLLMServiceHandler) LoadModel(context.Context, *connect.Request[llm.LoadModelRequest]) (*connect.Response[llm.LoadModelResponse], error) {
	return nil, connect.NewError(connect.CodeUnimplemented, errors.New("llm.v1beta1.LLMService.LoadModel is not implemented"))
}
