syntax = "proto3";
package llm.v1;

import "google/api/annotations.proto";
import "buf/validate/validate.proto";
import "google/protobuf/empty.proto";
import "google/protobuf/struct.proto";

service LLMService {
  rpc ChatCompletions(CompletionRequest) returns (CompletionResponse) {
    option (google.api.http).post = "/v1/chat/completions";
    option (google.api.http).body = "*";
  }
  rpc StreamChatCompletions(CompletionRequest) returns (stream CompletionResponse) {
    option (google.api.http).post = "/v1/chat/completions:stream";
    option (google.api.http).body = "*";
  }
}

enum FinishReason {
  NULL = 0;
  LENGTH = 1;
  STOP = 2;
  ERROR = 3;
}

message Role {
  oneof role {
    string system = 1;
    string user = 2;
    string assistant = 3;
  }
}

message ResponseFormat {
  string type = 1;
}

message ChatMessage {
  // role of the message author. One of "system", "user", "assistant".
  string role = 1 [(buf.validate.field).required = true];
  // content of the message
  string content = 2 [(buf.validate.field).required = true];
  google.protobuf.Struct logprobs = 3;
}



message CompletionRequest {
  string model = 1 [(buf.validate.field).required = true];
  // a list of messages comprising all the conversation so far
  repeated ChatMessage messages = 2 [(buf.validate.field).repeated.min_items = 1];

  // temperature of the sampling, between [0, 2]. default = 1.0
  optional float temperature = 3;

  optional uint32 seed = 4;
  // number of chat completion choices to generate for each input message. default = 1
  optional uint32 n = 5;
  optional float presence_penalty = 6 [(buf.validate.field).float.gt = -2.0, (buf.validate.field).float.lt = 2.0];
  optional float frequency_penalty = 7 [(buf.validate.field).float.gt = -2.0, (buf.validate.field).float.lt = 2.0];
  
  // whether to stream partial completions back as they are generated. default = false
  optional bool stream = 8;
  optional uint32 top_k = 9;
  optional float top_p = 10;
  repeated string stop = 11;
  optional uint64 max_tokens = 12;
  optional bool logprobs = 13;
  optional uint32 top_logprobs=14;
  optional google.protobuf.Struct logit_bias = 15;
  optional ResponseFormat response_format = 16;
}

message CompletionChoice {
  // index of the choice in the list of choices.
  uint32 index = 1;
  // message generated by the model.
  ChatMessage message = 2;
  google.protobuf.Struct logprobs = 3;
  string finish_reason = 4;
}

message Usage {
  // number of tokens in the prompt.
  optional int32 prompt_tokens = 1;

  // number of tokens in the generated completion.
  optional int32 completion_tokens = 2;

  // total number of tokens used in the request (prompt + completion).
  optional int32 total_tokens = 3;
}

message CompletionResponse {
  // unique id for the chat completion.
  string id = 1;
  // object type, which is always "chat.completion[.chunk]".
  string object = 2;
  // unix timestamp (in seconds) of when the chat completion was created.
  uint64 created = 3;
  // model used for the completion
  string model = 4;
  // list of generated completion choices for the input prompt
  repeated CompletionChoice choices = 5;
  // usage statistics for the completion request.
  Usage usage = 6;
  string system_fingerprint = 7;
}
